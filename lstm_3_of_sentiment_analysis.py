# -*- coding: utf-8 -*-
"""LSTM_3_of_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DH3vFWvxyOKEdcbBw4MdjSR51Foy4ksg

# PROSES 1: IMPORT LIBRARIES
"""

# ===== SEL 1: IMPORT LIBRARIES =====
# Import semua library yang diperlukan
import os
from pyexpat import features
import re
import json
import time
import pickle
import warnings
import logging
from typing import Dict, List, Tuple, Union

# Data Handling
import pandas as pd
import numpy as np
from collections import Counter

# Progress tracking
from tqdm import tqdm
tqdm.pandas()

# NLP & ML
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from scipy.sparse import hstack

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D, concatenate, Reshape
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Configuration
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("="*80)
print("IMPORTS COMPLETED SUCCESSFULLY!")
print("TensorFlow version:", tf.__version__)
print("="*80)

"""# PROSES 2: DOWNLOAD NLTK RESOURCES"""

# ===== SEL 2: DOWNLOAD NLTK RESOURCES =====
# Download NLTK resources yang diperlukan
print("Downloading NLTK resources...")
for resource in ["punkt", "stopwords", "wordnet"]:
    print(f"Downloading {resource}...")
    nltk.download(resource)
print("NLTK resources downloaded successfully!")

"""# PROSES 3: CONFIGURATION"""

# ===== SEL 3: CONFIGURATION =====
# Konfigurasi parameter model dan training
class Config:
    # Paths
    DATA_PATH = "/content/drive/MyDrive/Colab Notebooks/stock-price-prediction-sentiment-analysis/data.csv"
    MODEL_SAVE_PATH = "financial_sentiment_model_unified.pkl"  # Single unified model file

    # Text Processing
    MAX_LENGTH = 150  # Increased to capture more context
    MAX_VOCAB_SIZE = 20000  # Increased vocabulary size for better coverage

    # Model Parameters
    SEED = 42
    TEST_SIZE = 0.15
    VAL_SIZE = 0.1
    BATCH_SIZE = 64  # Increased batch size for more stable training

    # LSTM Parameters (optimized)
    EMBEDDING_DIM = 200  # Increased for better representation
    LSTM_UNITS = 128  # Increased LSTM units
    DROPOUT_RATE = 0.3  # Optimized dropout
    SPATIAL_DROPOUT = 0.15  # Optimized spatial dropout
    EPOCHS = 50  # Increased epochs for better convergence
    LEARNING_RATE = 0.0001  # Will be halved in the model for more stable training

    # Training enhancements
    EARLY_STOPPING_PATIENCE = 10  # Increased patience
    REDUCE_LR_PATIENCE = 3  # Increased patience
    REDUCE_LR_FACTOR = 0.5
    L2_REGULARIZATION = 0.0005  # Reduced for less overfitting

    # TF-IDF configuration
    TFIDF_MAX_FEATURES = 5000  # Doubled for more features
    TFIDF_NGRAM_RANGE = (1, 2)  # Extended to trigrams
    CLASS_WEIGHTS = {0: 3.0, 1: 1.0, 2: 2.0}

# Set seed for reproducibility
np.random.seed(Config.SEED)
tf.random.set_seed(Config.SEED)
os.environ['PYTHONHASHSEED'] = str(Config.SEED)

print("="*80)
print("CONFIGURATION INITIALIZED:")
print(f"- Max sequence length: {Config.MAX_LENGTH}")
print(f"- Max vocabulary size: {Config.MAX_VOCAB_SIZE}")
print(f"- Embedding dimension: {Config.EMBEDDING_DIM}")
print(f"- LSTM units: {Config.LSTM_UNITS}")
print(f"- Batch size: {Config.BATCH_SIZE}")
print(f"- Learning rate: {Config.LEARNING_RATE}")
print(f"- Epochs: {Config.EPOCHS}")
print(f"- Early stopping patience: {Config.EARLY_STOPPING_PATIENCE}")
print("="*80)

"""# PROSES 4: TEXT PREPROCESSOR"""

# ===== SEL 4: TEXT PREPROCESSOR =====
# Kelas untuk preprocessing teks finansial

class FinancialTextPreprocessor:
    """Financial text preprocessing with domain-specific handling"""

    def __init__(self):
        print("Initializing Financial Text Preprocessor...")
        self.lemmatizer = WordNetLemmatizer()
        self.tokenizer = RegexpTokenizer(r"\b[a-zA-Z$%]{2,}\b")

        # Enhanced financial stopwords - carefully selected to maintain sentiment
        self.financial_stopwords = [
            "not", "no", "nor", "up", "down", "above", "below",
            "increase", "decrease", "higher", "lower", "more", "less",
            "growth", "gain", "loss", "rise", "fell", "fall",
        ]
        self.stop_words = list(set(stopwords.words("english")) - set(self.financial_stopwords))

        # Enhanced special tokens with more financial indicators
        self.special_tokens = {
            "$": "dollar",
            "%": "percent",
            "€": "euro",
            "£": "pound",
            "¥": "yen",
            "+": "plus",
            "-": "minus",
            "¢": "cent",
            "&": "and",
            "™": "trademark",
            "®": "registered",
            "©": "copyright",
            "bbl": "barrel",
            "oz": "ounce",
            "m2": "square_meter",
            "m3": "cubic_meter",
            "bps": "basis_points",
            "k": "thousand",
            "m": "million",
            "b": "billion",
            "t": "trillion",
            "pct": "percent",
            "usd": "dollar",
            "eur": "euro",
            "gbp": "pound",
            "jpy": "yen",
            "cny": "yuan",
            "inr": "rupee",
            "krw": "won",
            "brl": "real",
            "aud": "australian_dollar",
            "cad": "canadian_dollar",
            "chf": "swiss_franc",
            "sek": "swedish_krona",
            "nok": "norwegian_krone",
            "dkk": "danish_krone",
            "sgd": "singapore_dollar",
        }

        # Financial regex patterns with improved recognition
        self.patterns = {
            "tickers": re.compile(r"\$?([A-Z]{1,5})\b"),  # Improved ticker detection, including $TICK format
            "numbers": re.compile(r"\b\d+(?:\.\d+)?%?\b"),
            "currencies": re.compile(r"[\$€£¥]\d+(?:\.\d+)?[KMBTkmbt]?\b"),  # Added K,M,B,T suffixes
            "percentages": re.compile(r"\b\d+(?:\.\d+)?%\b"),
            "dates": re.compile(r"\b(?:19|20)\d{2}[/-](?:0?[1-9]|1[0-2])[/-](?:0?[1-9]|[12][0-9]|3[01])\b"),
            "long_numbers": re.compile(r"\b\d{1,3}(,\d{3})+(\.\d+)?\b"),  # 1,000 or 1,000,000.00
            "abbreviated_currency": re.compile(r"[\$€£¥]\s?\d+[KMBT]"),  # $5M, €2B etc.
            "fiscal_quarters": re.compile(r"\bQ[1-4]\s?[-]?\s?(?:\d{2}|\d{4})\b", re.IGNORECASE),
            "financial_ratios": re.compile(r"\b\d+:\d+\b"),  # like 3:1 debt ratio
            "ticker_mentions": re.compile(r"\$\b[A-Z]{1,5}\b")  # e.g. $AAPL,
        }

        # Expanded financial phrase preservation
        self.financial_phrases = [
            # Market terms
            "fed rate", "interest rate", "earnings call", "balance sheet",
            "cash flow", "market cap", "bull market", "bear market",
            "stock market", "profit margin", "revenue growth", "quarterly report",
            "dividend payout", "return on investment", "price to earnings",
            "economic indicator", "fiscal policy", "monetary policy",

            # Performance indicators
            "year over year", "quarter over quarter", "missed expectations",
            "beat expectations", "raised guidance", "lowered guidance",
            "earnings per share", "eps", "price target", "trading volume",

            # Market events
            "market crash", "stock split", "ipo", "merger acquisition",
            "buyback", "insider trading", "sec filing", "short squeeze",

            # Financial health
            "debt ratio", "liquidity", "solvency", "bankruptcy risk",
            "credit rating", "default risk", "free cash flow",

            # Others
             "gross domestic product", "consumer confidence", "jobless claim",
            "interest coverage", "price to book", "return on equity", "net interest margin",
            "price to sales", "debt to equity", "working capital", "current ratio",
            "quick ratio", "earnings yield", "cash equivalent", "operating cash flow",
            "free float", "hedge fund", "private equity", "venture capital",
            "initial public offering", "capital expenditure", "supply chain disruption",
            "inventory turnover", "capital structure", "operating income",
            "leveraged buyout", "credit default swap", "nonperforming loan",
            "total return", "cost of capital", "foreign exchange reserve", "capital adequacy",
            "asset quality", "risk weighted asset", "capital requirement", "capital buffer",
            "capital surplus", "capital adequacy ratio", "capital efficiency", "capital allocation",
            "capital budgeting", "capital investment", "capital expenditure ratio", "capital intensive", "capital constrained",
            "capital intensive industry", "capital intensive business", "capital intensive project", "capital intensive operation",
            "capital intensive activity", "capital intensive sector", "capital intensive company", "capital intensive enterprise", "capital intensive organization",
        ]

        # Significantly expanded financial sentiment lexicons
        self.positive_financial_terms = [
            # Growth and performance
            "growth", "profit", "gain", "increase", "improve", "positive", "strong",
            "opportunity", "up", "higher", "rise", "rising", "boost", "bullish",
            "optimistic", "exceed", "outperform", "beat", "recovery", "advantage",
            "success", "growing", "strengthen", "upgrade", "optimize", "beneficial",
            "innovative", "efficient", "momentum", "upside", "robust",

            # Market sentiment
            "confident", "favorable", "promising", "encouraging", "impressive",
            "surpass", "rally", "rebound", "uptrend", "breakthrough", "expansion",

            # Company operations
            "streamlined", "cost-saving", "synergy", "strategic", "efficient",
            "diversified", "well-positioned", "market-leading", "competitive",

            # Financial metrics
            "profitable", "dividend", "earnings-beat", "revenue-growth",
            "margin-expansion", "cash-rich", "debt-reduction", "balance-sheet-strength",

            # Analyst perspectives
            "overweight", "buy-rating", "accumulate", "target-price-raise",

            # Others
            "positive_outlook", "bullish_sentiment", "bullish_outlook", "bullish_forecast", "bullish_prediction", "bullish_projection", "bullish_assessment", "bullish_evaluation", "bullish_analysis", "bullish_interpretation", "bullish_conclusion",  "accretion", "accretive", "appreciation", "capital_gain", "cash_surplus",
            "clear_profit", "cost_control", "credit_upgrade", "earnings_growth",
            "eps_growth", "favorable_outlook", "favorable_guidance", "gdp_growth",
            "gross_margin", "healthy_balance", "high_roi", "improved_margins",
            "in_the_black", "liquidity_boost", "margin_improvement", "net_profit",
            "positive_cash_flow", "pricing_power", "profit_growth", "record_earnings",
            "record_high", "revenue_boost", "shareholder_gain", "top_line_growth",
            "turnaround", "value_increase", "yield_increase",
        ]

        self.negative_financial_terms = [
            # Decline terms
            "loss", "decline", "decrease", "drop", "fall", "negative", "weak",
            "risk", "down", "lower", "bearish", "threat", "pessimistic", "underperform",
            "miss", "downturn", "disadvantage", "failure", "recession", "struggle",
            "crisis", "debt", "bankruptcy", "downgrade", "liability", "deficit",
            "concern", "volatile", "crash", "plunge", "adverse", "slump",

            # Company problems
            "layoff", "restructuring", "write-down", "impairment", "investigation",
            "litigation", "lawsuit", "fine", "penalty", "scandal", "controversy",

            # Market conditions
            "sell-off", "correction", "bear-market", "downturn", "contraction",
            "slowdown", "headwind", "pressure", "challenging", "difficult",

            # Financial health
            "debt-burden", "cash-burn", "margin-compression", "revenue-miss",
            "guidance-cut", "earnings-miss", "liquidity-concern", "default-risk",

            # Analyst perspectives
            "underweight", "sell-rating", "reduce", "target-price-cut",

            # Others
             "accounting_irregularity", "asset_writeoff", "bailout", "bad_debt",
            "below_estimates", "budget_deficit", "capital_loss", "cash_crunch",
            "credit_downgrade", "default", "dilution", "earnings_decline",
            "earnings_warning", "eps_drop", "excessive_leverage", "exposure_risk",
            "financial_irregularity", "fiscal_deficit", "gdp_contraction",
            "going_concern", "in_the_red", "inventory_glut", "layoffs", "liquidity_crisis",
            "margin_decline", "missed_estimates", "negative_cash_flow", "operating_loss",
            "profit_warning", "rating_downgrade", "revenue_decline", "shareholder_loss",
            "underperformance", "weak_guidance", "write_down", "writedown", "shortfall", "shortage", "shortage_risk", "shortage_concern", "shortage_threat", "shortage_pressure", "shortage_challenge", "shortage_difficulty", "shortage_problem", "shortage_issue", "shortage_obstacle", "shortage_hurdle",
        ]

        # NEW: Negation modifiers that flip sentiment
        self.negation_terms = [
            "not", "no", "never", "neither", "nor", "none", "nothing",
            "cannot", "can't", "won't", "wouldn't", "couldn't", "shouldn't",
            "don't", "doesn't", "didn't", "isn't", "aren't", "wasn't", "weren't",
            "without", "lack", "lacking", "failed", "failure", "avoid", "avoiding", "unprofitable", "unsuccessful", "underperforming", "unable", "uncertain",
            "unstable", "noncompliant", "unrealized", "unearned", "unreliable", "doubtful", "questionable", "suspect", "suspicious", "dubious", "contradictory", "contradiction", "contradicted", "contradicting", "contradicts", "contradicted_by", "contradicted_to", "contradicted_against", "contradicted_from",
        ]

        # NEW: Intensity modifiers that strengthen sentiment
        self.intensity_modifiers = {
            "very": 1.5,
            "extremely": 2.0,
            "highly": 1.8,
            "significantly": 1.7,
            "substantially": 1.7,
            "considerable": 1.5,
            "major": 1.6,
            "massive": 1.9,
            "tremendous": 1.8,
            "exceptional": 1.7,
            "sharp": 1.6,
            "severe": 1.8,
            "dramatic": 1.7,
            "slight": 0.5,
            "minor": 0.4,
            "modest": 0.7,
            "marginal": 0.3,
            "barely": 0.2,
            "somewhat": 0.6,
             "insanely": 2.0,
            "ridiculously": 1.9,
            "remarkably": 1.7,
            "extraordinarily": 2.2,
            "unusually": 1.4,
            "awfully": 1.6,
            "terribly": 1.5,
            "exceptionally": 1.9,
            "slightly": 0.4,
            "noticeably": 1.3,
            "materially": 1.5,
            "mildly": 0.5,
            "drastically": 2.0,
            "noticeable": 1.3,
        }

        print("Financial Text Preprocessor initialized successfully!")
        print(f"- Preserved financially relevant terms: {len(self.financial_phrases)} phrases")
        print(f"- Maintained financial stopwords: {', '.join(sorted(self.financial_stopwords))}")
        print(f"- Positive financial terms: {len(self.positive_financial_terms)} terms")
        print(f"- Negative financial terms: {len(self.negative_financial_terms)} terms")
        print(f"- Negation terms: {len(self.negation_terms)}")
        print(f"- Intensity modifiers: {len(self.intensity_modifiers)}")

    def _preserve_financial_phrases(self, text: str) -> str:
        """Preserve important financial multi-word expressions"""
        for phrase in self.financial_phrases:
            text = text.replace(phrase, phrase.replace(" ", "_"))
        return text

    def _handle_special_tokens(self, text: str) -> str:
        """Process financial special tokens"""
        for token, replacement in self.special_tokens.items():
            text = text.replace(token, f" {replacement} ")
        return text

    def _process_numbers(self, text: str) -> str:
        """Normalize numerical expressions"""
        # Percentages
        text = re.sub(r"(\d+)%", r"\1 percent", text)
        # Preserve dollar amounts
        text = re.sub(r"\$(\d+)", r"dollar \1", text)
        # Handle K, M, B, T suffixes
        text = re.sub(r"(\d+)([KMBTkmbt])\b", lambda m:
                     f"{m.group(1)} {m.group(2).lower()}", text)
        return text

    def _handle_negations(self, text: str) -> str:
        """Handle negations for sentiment analysis with improved contextual understanding"""
        # Find all negation terms
        for negation in self.negation_terms:
            # Find all negation term occurrences
            negation_pattern = re.compile(r'\b' + re.escape(negation) + r'\b')
            matches = list(negation_pattern.finditer(text))

            # Process each match from end to start (to avoid offset issues when replacing)
            for match in reversed(matches):
                # Look for positive or negative sentiment words in a 5-word window after the negation
                start_pos = match.end()
                text_after = text[start_pos:].split()[:5]  # Take up to 5 words after negation

                # Check if any of these words are sentiment words
                for i, word in enumerate(text_after):
                    word_clean = re.sub(r'[^\w]', '', word.lower())
                    if word_clean in self.positive_financial_terms:
                        # Replace the positive word with a marked version to indicate it's negated
                        text_after[i] = 'neg_' + word_clean
                        break
                    elif word_clean in self.negative_financial_terms:
                        # Replace the negative word with a marked version to indicate it's negated
                        text_after[i] = 'neg_' + word_clean
                        break

                # Replace the original segment with the modified version
                modified_segment = text[:start_pos] + ' ' + ' '.join(text_after)
                remaining_text = text[start_pos:].split(' ', 5+1)[5+1:] if len(text[start_pos:].split(' ')) > 5 else []
                text = modified_segment + (' ' + ' '.join(remaining_text) if remaining_text else '')

        return text

    # Enhanced sentiment features extraction
    def extract_sentiment_features(self, text: str) -> dict:
        """Extract rich sentiment features from the text with contextual understanding"""
        text_lower = text.lower()
        words = text_lower.split()

        # Count standard sentiment terms
        positive_count = 0
        negative_count = 0

        # Track which terms were found for debugging
        found_positive_terms = []
        found_negative_terms = []
        found_negated_terms = []
        found_intensified_terms = []

        # First pass: identify all terms
        for word in words:
            clean_word = re.sub(r'[^\w]', '', word)

            # Check for negated sentiment terms
            if clean_word.startswith('neg_'):
                base_word = clean_word[4:]  # Remove 'neg_' prefix
                found_negated_terms.append(base_word)

                # Flip sentiment for negated words
                if base_word in self.positive_financial_terms:
                    negative_count += 1
                elif base_word in self.negative_financial_terms:
                    positive_count += 1

            # Count direct sentiment terms
            elif clean_word in self.positive_financial_terms:
                positive_count += 1
                found_positive_terms.append(clean_word)
            elif clean_word in self.negative_financial_terms:
                negative_count += 1
                found_negative_terms.append(clean_word)

        # Second pass: analyze intensity modifiers
        intensity_score = 0
        for i, word in enumerate(words):
            if i < len(words) - 1:  # Ensure we have a next word
                clean_word = re.sub(r'[^\w]', '', word)
                next_word = re.sub(r'[^\w]', '', words[i+1])

                # Check if current word is an intensity modifier
                if clean_word in self.intensity_modifiers:
                    modifier_value = self.intensity_modifiers[clean_word]

                    # Check if the next word is a sentiment word that can be modified
                    if next_word in self.positive_financial_terms:
                        positive_count += (modifier_value - 1)  # Add extra weight
                        found_intensified_terms.append(f"{clean_word}_{next_word}")
                    elif next_word in self.negative_financial_terms:
                        negative_count += (modifier_value - 1)  # Add extra weight
                        found_intensified_terms.append(f"{clean_word}_{next_word}")

        # Calculate enhanced sentiment metrics
        sentiment_score = positive_count - negative_count  # Basic differential
        sentiment_intensity = positive_count + negative_count  # Total sentiment volume
        weighted_score = sentiment_score / max(sentiment_intensity, 1)  # Normalized between -1 and 1

        # Create additional features from text analysis
        has_exclamation = '!' in text
        question_count = text.count('?')
        all_caps_words = sum(1 for word in text.split() if word.isupper() and len(word) > 1)

        # Check for price movement indicators
        up_indicators = sum(1 for pattern in ['increase', 'rise', 'gain', 'higher', 'up']
                          if pattern in text_lower)
        down_indicators = sum(1 for pattern in ['decrease', 'fall', 'drop', 'lower', 'down']
                            if pattern in text_lower)

        return {
            "positive_term_count": positive_count,
            "negative_term_count": negative_count,
            "sentiment_score": sentiment_score,
            "sentiment_intensity": sentiment_intensity,
            "sentiment_polarity": weighted_score,
            "has_exclamation": int(has_exclamation),
            "question_count": min(question_count, 3),  # Cap at 3 for normalization
            "all_caps_count": min(all_caps_words, 5),  # Cap at 5 for normalization
            "up_indicators": up_indicators,
            "down_indicators": down_indicators,
            # Diagnostics (helpful for debugging)
            "found_positive": ",".join(found_positive_terms[:5]),
            "found_negative": ",".join(found_negative_terms[:5]),
            "found_negated": ",".join(found_negated_terms[:5]),
            "found_intensified": ",".join(found_intensified_terms[:5]),
        }

    def preprocess(self, text: str) -> str:
        """Full preprocessing pipeline with improved financial context understanding"""
        if pd.isna(text):
            return ""

        text = str(text).lower()

        # Step 1: Preserve financial phrases
        text = self._preserve_financial_phrases(text)

        # Step 2: Handle special tokens
        text = self._handle_special_tokens(text)

        # Step 3: Process numerical expressions
        text = self._process_numbers(text)

        # Step 4: Handle negations with improved context
        text = self._handle_negations(text)

        # Step 5: Remove URLs and HTML
        text = re.sub(r"https?://\S+|www\.\S+", "", text)
        text = re.sub(r"<.*?>", "", text)

        # Step 6: Enhanced tokenization - preserve important financial symbols
        tokens = self.tokenizer.tokenize(text)

        # Step 7: Lemmatization
        lemmatized = [self.lemmatizer.lemmatize(word) for word in tokens]

        # Step 8: Stopword removal preserving negation and financial terms
        filtered = [
            word for word in lemmatized
            if (word not in self.stop_words and len(word) > 1) or
               word.startswith('neg_') or
               any(term in word for term in ['up', 'down', 'rise', 'fall', 'dollar'])
        ]

        return " ".join(filtered)

# Test preprocessor with a sample text
preprocessor = FinancialTextPreprocessor()
sample_text = "The company reported a 15% increase in quarterly earnings, with $500 million in revenue. The stock market reacted positively."
processed_text = preprocessor.preprocess(sample_text)
print("\nSample preprocessing:")
print(f"Original: \"{sample_text}\"")
print(f"Processed: \"{processed_text}\"")
print("="*80)

"""# PROSES 5: LSTM MODEL CLASS"""

# ===== SEL 5: LSTM MODEL CLASS =====
# Kelas model LSTM untuk analisis sentimen finansial dengan integrasi TF-IDF
# Improved FinancialSentimentLSTM class with single-file serialization

class FinancialSentimentLSTM:
    """Financial sentiment analysis using optimized LSTM architecture with TF-IDF features and single file serialization"""

    def __init__(self):
        print("="*80)
        print("[INFO] Initializing Financial Sentiment LSTM model...")
        self.preprocessor = FinancialTextPreprocessor()
        self.tokenizer = None
        self.tfidf_vectorizer = None
        self.model = None
        self.max_sequence_length = Config.MAX_LENGTH
        self.max_vocab_size = Config.MAX_VOCAB_SIZE
        self.is_fitted = False

        # NEW: Add rules for post-processing
        self.rules = {
            "positive_keywords": [
                "growth", "profit", "increase", "improve", "positive", "strong",
                "up", "higher", "rise", "rising", "boost", "bullish", "exceed",
                "outperform", "beat", "recovery", "success", "growing",
                "strengthen", "upgrade", "optimize", "beneficial"
            ],
            "negative_keywords": [
                "loss", "decline", "decrease", "drop", "fall", "negative", "weak",
                "down", "lower", "threat", "miss", "downturn", "failure", "struggle",
                "crisis", "debt", "bankruptcy", "downgrade", "liability", "deficit"
            ],
            "confidence_threshold": 0.55  # Threshold for applying post-processing rules
        }

        print("[INFO] Financial Sentiment LSTM model initialized")
        print("="*80)

    def _build_advanced_lstm_model(self, vocab_size: int, num_classes: int, tfidf_dim: int) -> Model:
        """Build an optimized bidirectional LSTM model with TF-IDF features and sentiment features"""
        print("\n[MODEL ARCHITECTURE] Building advanced Bidirectional LSTM model with TF-IDF and sentiment features...")
        print(f"[MODEL ARCHITECTURE] Vocabulary size: {vocab_size}")
        print(f"[MODEL ARCHITECTURE] Number of classes: {num_classes}")
        print(f"[MODEL ARCHITECTURE] Embedding dimension: {Config.EMBEDDING_DIM}")
        print(f"[MODEL ARCHITECTURE] LSTM units: {Config.LSTM_UNITS}")
        print(f"[MODEL ARCHITECTURE] TF-IDF dimension: {tfidf_dim}")

        # Text input and embedding branch
        text_input = Input(shape=(Config.MAX_LENGTH,), name='text_input')
        embedding = Embedding(input_dim=vocab_size,
                             output_dim=Config.EMBEDDING_DIM,
                             input_length=Config.MAX_LENGTH,
                             name='embedding_layer')(text_input)
        embedding = SpatialDropout1D(Config.SPATIAL_DROPOUT, name='spatial_dropout')(embedding)

        # Bidirectional LSTM layers with increased capacity
        lstm = Bidirectional(LSTM(Config.LSTM_UNITS,
                                 return_sequences=True,
                                 kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                                 recurrent_regularizer=l2(Config.L2_REGULARIZATION/2),
                                 name='bidirectional_lstm_1'))(embedding)
        lstm = Dropout(Config.DROPOUT_RATE, name='dropout_lstm_1')(lstm)

        lstm = Bidirectional(LSTM(Config.LSTM_UNITS,
                                 kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                                 recurrent_regularizer=l2(Config.L2_REGULARIZATION/2),
                                 name='bidirectional_lstm_2'))(lstm)
        lstm = Dropout(Config.DROPOUT_RATE, name='dropout_lstm_2')(lstm)

        # TF-IDF input branch with improved architecture
        tfidf_input = Input(shape=(tfidf_dim,), name='tfidf_input')
        tfidf_dense = Dense(256, activation='relu',
                           kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                           name='tfidf_dense_1')(tfidf_input)
        tfidf_dropout = Dropout(Config.DROPOUT_RATE/2, name='tfidf_dropout_1')(tfidf_dense)

        tfidf_dense2 = Dense(128, activation='relu',
                           kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                           name='tfidf_dense_2')(tfidf_dropout)
        tfidf_dropout2 = Dropout(Config.DROPOUT_RATE/2, name='tfidf_dropout_2')(tfidf_dense2)

        # NEW: Additional sentiment features input branch
        # Updated sentiment input shape (10 features)
        sentiment_input = Input(shape=(10,), name='sentiment_features_input')
        sentiment_dense = Dense(32, activation='relu',
                              kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                              name='sentiment_dense_1')(sentiment_input)
        sentiment_dropout = Dropout(Config.DROPOUT_RATE/3, name='sentiment_dropout_1')(sentiment_dense)

        # Concatenate all branches
        concatenated = concatenate([lstm, tfidf_dropout2, sentiment_dropout], name='concatenated')

        # Dense layers for classification with improved architecture
        dense = Dense(512, activation='relu',
                     kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                     name='dense_1')(concatenated)
        dense = Dropout(Config.DROPOUT_RATE, name='dropout_dense_1')(dense)

        dense = Dense(256, activation='relu',
                     kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                     name='dense_2')(dense)
        dense = Dropout(Config.DROPOUT_RATE/2, name='dropout_dense_2')(dense)

        dense = Dense(128, activation='relu',
                     kernel_regularizer=l2(Config.L2_REGULARIZATION/2),
                     name='dense_3')(dense)
        dense = Dropout(Config.DROPOUT_RATE/3, name='dropout_dense_3')(dense)

        output = Dense(num_classes, activation='softmax', name='output_layer')(dense)

        # Compile the model with reduced learning rate for better convergence
        model = Model(inputs=[text_input, tfidf_input, sentiment_input], outputs=output)
        optimizer = Adam(learning_rate=Config.LEARNING_RATE/2)
        model.compile(optimizer=optimizer,
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])

        print("[MODEL ARCHITECTURE] Full model summary:")
        model.summary()
        return model

    def fit(self, X: Union[pd.Series, List[str]], y: pd.Series):
        """Train the LSTM model with advanced features and TF-IDF integration"""
        print("\n" + "="*80)
        print("[TRAINING] Starting LSTM model training process with TF-IDF and sentiment features...")
        print("="*80)

        # Convert to list if Series
        texts = X.tolist() if isinstance(X, pd.Series) else X
        print(f"[TRAINING] Training set size: {len(texts)} samples")
        print(f"[TRAINING] Class distribution: {pd.Series(y).value_counts().to_dict()}")

        # Preprocess texts
        print("\n[PREPROCESSING] Starting text preprocessing...")
        start_preprocess = time.time()
        processed_texts = [self.preprocessor.preprocess(text) for text in tqdm(texts, desc="Preprocessing texts")]
        preprocess_time = time.time() - start_preprocess
        print(f"[PREPROCESSING] Text preprocessing completed in {preprocess_time:.2f} seconds")

        # Extract sentiment features
        # Extract sentiment features (first 10 numerical values)
        print("\n[FEATURE EXTRACTION] Extracting sentiment features...")
        start_feature = time.time()
        sentiment_features = np.array([
            list(self.preprocessor.extract_sentiment_features(text).values())[:10]
            for text in tqdm(texts, desc="Extracting sentiment features")
        ], dtype=np.float32)
        feature_time = time.time() - start_feature
        print(f"[FEATURE EXTRACTION] Sentiment features extracted in {feature_time:.2f} seconds")
        print(f"[FEATURE EXTRACTION] Sentiment feature shape: {sentiment_features.shape}")

        # Initialize and fit tokenizer
        print("\n[TOKENIZATION] Initializing and fitting tokenizer...")
        start_tokenize = time.time()
        self.tokenizer = Tokenizer(num_words=self.max_vocab_size, oov_token="<UNK>")
        self.tokenizer.fit_on_texts(processed_texts)
        tokenize_time = time.time() - start_tokenize

        # Display tokenizer statistics
        print(f"[TOKENIZATION] Tokenizer fitted in {tokenize_time:.2f} seconds")
        print(f"[TOKENIZATION] Total unique words found: {len(self.tokenizer.word_index)}")
        print(f"[TOKENIZATION] Using top {self.max_vocab_size} words in vocabulary")

        # Display most common words
        word_counts = sorted(self.tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)
        print(f"[TOKENIZATION] Top 10 most frequent words in corpus:")
        for word, count in word_counts[:10]:
            print(f"  - '{word}': {count} occurrences")

        # Convert texts to sequences and pad
        print("\n[SEQUENCE PADDING] Converting texts to sequence and padding...")
        start_seq = time.time()
        sequences = self.tokenizer.texts_to_sequences(processed_texts)
        X_text = pad_sequences(sequences, maxlen=self.max_sequence_length, padding='post')
        seq_time = time.time() - start_seq
        print(f"[SEQUENCE PADDING] Sequences created and padded in {seq_time:.2f} seconds")
        print(f"[SEQUENCE PADDING] Padded sequence shape: {X_text.shape}")

        # Initialize and fit TF-IDF vectorizer with improved params
        print("\n[TF-IDF] Initializing and fitting TF-IDF vectorizer...")
        start_tfidf = time.time()
        self.tfidf_vectorizer = TfidfVectorizer(
        max_features=Config.TFIDF_MAX_FEATURES * 2,
        ngram_range=(1, 3),
        min_df=2,
        max_df=0.95,
        sublinear_tf=True,
        stop_words=list(self.preprocessor.stop_words)
        )
        X_tfidf = self.tfidf_vectorizer.fit_transform(processed_texts).astype(np.float32)
        tfidf_time = time.time() - start_tfidf
        print(f"[TF-IDF] TF-IDF features extracted in {tfidf_time:.2f} seconds")
        print(f"[TF-IDF] TF-IDF feature dimension: {X_tfidf.shape[1]}")

        # NEW: Compute class weights to handle class imbalance
        print("\n[CLASS BALANCING] Computing class weights to handle imbalance...")
        class_counts = pd.Series(y).value_counts().sort_index()
        n_samples = len(y)
        n_classes = len(class_counts)
        class_weights = {i: n_samples / (n_classes * count) for i, count in enumerate(class_counts)}
        print(f"[CLASS BALANCING] Class weights: {class_weights}")

        # Split into train and validation sets
        print("\n[DATA SPLIT] Splitting data into training and validation sets...")
        X_train_text, X_val_text, X_train_tfidf, X_val_tfidf, X_train_sentiment, X_val_sentiment, y_train, y_val = train_test_split(
            X_text, X_tfidf, sentiment_features, y, test_size=Config.VAL_SIZE, stratify=y, random_state=Config.SEED
        )
        print(f"[DATA SPLIT] Training set shape (text): {X_train_text.shape}")
        print(f"[DATA SPLIT] Training set shape (TF-IDF): {X_train_tfidf.shape}")
        print(f"[DATA SPLIT] Training set shape (sentiment features): {X_train_sentiment.shape}")
        print(f"[DATA SPLIT] Validation set shape (text): {X_val_text.shape}")
        print(f"[DATA SPLIT] Validation set shape (TF-IDF): {X_val_tfidf.shape}")
        print(f"[DATA SPLIT] Validation set shape (sentiment features): {X_val_sentiment.shape}")
        print(f"[DATA SPLIT] Training class distribution: {pd.Series(y_train).value_counts().to_dict()}")
        print(f"[DATA SPLIT] Validation class distribution: {pd.Series(y_val).value_counts().to_dict()}")

        # Build model
        print("\n[MODEL BUILD] Building the LSTM model with TF-IDF and sentiment features...")
        start_build = time.time()
        num_classes = len(set(y))
        vocab_size = min(len(self.tokenizer.word_index) + 1, self.max_vocab_size)
        self.model = self._build_advanced_lstm_model(vocab_size, num_classes, X_tfidf.shape[1])
        build_time = time.time() - start_build
        print(f"[MODEL BUILD] Model built in {build_time:.2f} seconds")

        # Setup callbacks for better training
        print("\n[CALLBACKS] Setting up training callbacks...")
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=Config.EARLY_STOPPING_PATIENCE,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=Config.REDUCE_LR_FACTOR,
                patience=Config.REDUCE_LR_PATIENCE,
                min_lr=1e-6,
                verbose=1
            ),
            ModelCheckpoint(
                'best_model_checkpoint.h5',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]
        print(f"[CALLBACKS] Early stopping with patience {Config.EARLY_STOPPING_PATIENCE}")
        print(f"[CALLBACKS] Learning rate reduction with factor {Config.REDUCE_LR_FACTOR} and patience {Config.REDUCE_LR_PATIENCE}")
        print(f"[CALLBACKS] Model checkpoint will save best weights based on validation accuracy")

        # Train the model with class weights
        print("\n[TRAINING] Starting model training with class weights...")
        print("[TRAINING] " + "="*60)
        print(f"[TRAINING] Total epochs: {Config.EPOCHS}")
        print(f"[TRAINING] Batch size: {Config.BATCH_SIZE}")
        print(f"[TRAINING] Training samples: {len(X_train_text)}")
        print(f"[TRAINING] Validation samples: {len(X_val_text)}")
        print(f"[TRAINING] Steps per epoch: {len(X_train_text) // Config.BATCH_SIZE}")
        print("[TRAINING] " + "="*60)

        start_time = time.time()
        history = self.model.fit(
            [X_train_text, X_train_tfidf.todense(), X_train_sentiment], y_train,
            validation_data=([X_val_text, X_val_tfidf.todense(), X_val_sentiment], y_val),
            epochs=Config.EPOCHS,
            batch_size=Config.BATCH_SIZE,
            callbacks=callbacks,
            class_weight=class_weights,  # Added class weights
            verbose=1
        )

        training_time = time.time() - start_time
        print("\n[TRAINING] " + "="*60)
        print(f"[TRAINING] Model trained in {training_time:.2f} seconds")
        print(f"[TRAINING] Average time per epoch: {training_time/len(history.history['loss']):.2f} seconds")

        # Training results visualization
        print("\n[TRAINING RESULTS] Training history summary:")
        print(f"[TRAINING RESULTS] Starting training loss: {history.history['loss'][0]:.4f}")
        print(f"[TRAINING RESULTS] Final training loss: {history.history['loss'][-1]:.4f}")
        print(f"[TRAINING RESULTS] Starting training accuracy: {history.history['accuracy'][0]:.4f}")
        print(f"[TRAINING RESULTS] Final training accuracy: {history.history['accuracy'][-1]:.4f}")
        print(f"[TRAINING RESULTS] Starting validation loss: {history.history['val_loss'][0]:.4f}")
        print(f"[TRAINING RESULTS] Final validation loss: {history.history['val_loss'][-1]:.4f}")
        print(f"[TRAINING RESULTS] Starting validation accuracy: {history.history['val_accuracy'][0]:.4f}")
        print(f"[TRAINING RESULTS] Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}")

        # Display learning rate changes
        if 'lr' in history.history:
            print("\n[TRAINING RESULTS] Learning rate changes:")
            for i, lr in enumerate(history.history['lr']):
                print(f"  - Epoch {i+1}: LR = {lr:.8f}")

        # Load the best model weights
        if os.path.exists('best_model_checkpoint.h5'):
            print("\n[MODEL] Loading best model weights from checkpoint...")
            self.model.load_weights('best_model_checkpoint.h5')
            print("[MODEL] Best weights loaded successfully")

        self.is_fitted = True
        print("\n[TRAINING] LSTM model training completed with TF-IDF and sentiment features!")
        print("="*80)
        return self

    def predict_proba(self, X: Union[pd.Series, List[str]]) -> np.ndarray:
        """Generate probability predictions using LSTM, TF-IDF, and sentiment features"""
        if not self.is_fitted:
            print("[ERROR] Model not fitted. Call fit() before prediction.")
            raise ValueError("The model has not been fitted yet")

        # Convert to list if Series
        texts = X.tolist() if isinstance(X, pd.Series) else X
        print(f"[PREDICTION] Generating predictions for {len(texts)} samples")

        # Preprocess texts
        print("[PREDICTION] Preprocessing input texts...")
        processed_texts = [self.preprocessor.preprocess(text) for text in texts]

        # Extract sentiment features
        print("[PREDICTION] Extracting sentiment features...")
        sentiment_features = np.array([
        list(self.preprocessor.extract_sentiment_features(text).values())[:10]
            for text in texts
        ], dtype=np.float32)

        # Convert to sequences and pad
        print("[PREDICTION] Converting to sequences and padding...")
        sequences = self.tokenizer.texts_to_sequences(processed_texts)
        X_text = pad_sequences(sequences, maxlen=self.max_sequence_length, padding='post')
        print(f"[PREDICTION] Input shape (text) after padding: {X_text.shape}")

        # Transform to TF-IDF features
        print("[PREDICTION] Transforming to TF-IDF features...")
        X_tfidf = self.tfidf_vectorizer.transform(processed_texts)
        print(f"[PREDICTION] Input shape (TF-IDF): {X_tfidf.shape}")
        print(f"[PREDICTION] Input shape (sentiment features): {sentiment_features.shape}")

        # Generate predictions
        print("[PREDICTION] Running model inference...")
        start_time = time.time()
        predictions = self.model.predict([X_text, X_tfidf.todense(), sentiment_features])
        pred_time = time.time() - start_time
        print(f"[PREDICTION] Predictions generated in {pred_time:.2f} seconds")
        print(f"[PREDICTION] Average prediction time per sample: {pred_time/len(texts)*1000:.2f} ms")

        return predictions

    def predict(self, X: Union[pd.Series, List[str]]) -> np.ndarray:
        """Generate class predictions with post-processing rules"""
        probas = self.predict_proba(X)

        # Convert to list if Series
        texts = X.tolist() if isinstance(X, pd.Series) else X

        # Basic predictions (argmax)
        predictions = np.argmax(probas, axis=1)

        # Apply post-processing rules
        print("[PREDICTION] Applying post-processing rules...")
        for i, (text, proba) in enumerate(zip(texts, probas)):
            text_lower = text.lower()
            confidence = proba[predictions[i]]

            # Only apply rules if confidence is below threshold
            if confidence < self.rules["confidence_threshold"]:
                # Check for positive keywords
                if any(keyword in text_lower for keyword in self.rules["positive_keywords"]):
                    if predictions[i] == 1:  # If currently neutral
                        predictions[i] = 2  # Change to positive
                        print(f"[RULES] Changed prediction from neutral to positive for: \"{text[:50]}...\"")

                # Check for negative keywords
                elif any(keyword in text_lower for keyword in self.rules["negative_keywords"]):
                    if predictions[i] == 1:  # If currently neutral
                        predictions[i] = 0  # Change to negative
                        print(f"[RULES] Changed prediction from neutral to negative for: \"{text[:50]}...\"")

        print(f"[RULES] Applied post-processing rules, modified {np.sum(np.argmax(probas, axis=1) != predictions)} predictions")
        return predictions

    def predict_single_text(self, text: str) -> dict:
        """Process a single text input and return detailed sentiment analysis"""
        if not self.is_fitted:
            raise ValueError("Model not fitted. Call fit() before prediction.")

        # Preprocess the input text
        processed_text = self.preprocessor.preprocess(text)

        # Extract sentiment features
        sentiment_features = np.array([
        list(self.preprocessor.extract_sentiment_features(text).values())[:10]
        ], dtype=np.float32)

        # Convert to sequence and pad
        sequence = self.tokenizer.texts_to_sequences([processed_text])
        X_text = pad_sequences(sequence, maxlen=self.max_sequence_length, padding='post')

        # Transform to TF-IDF features
        X_tfidf = self.tfidf_vectorizer.transform([processed_text])

        # Generate prediction
        start_time = time.time()
        probas = self.model.predict([X_text, X_tfidf.todense(), sentiment_features])
        prediction_time = time.time() - start_time

        # Get initial prediction
        sentiment_labels = ["negative", "neutral", "positive"]
        predicted_class = np.argmax(probas[0])
        confidence = float(probas[0][predicted_class])

        # Apply post-processing rules
        text_lower = text.lower()
        if confidence < self.rules["confidence_threshold"]:
            if any(keyword in text_lower for keyword in self.rules["positive_keywords"]) and predicted_class == 1:
                predicted_class = 2  # Change to positive

            elif any(keyword in text_lower for keyword in self.rules["negative_keywords"]) and predicted_class == 1:
                predicted_class = 0  # Change to negative

        # Return detailed results
        sentiment_info = self.preprocessor.extract_sentiment_features(text)
        return {
            "original_text": text,
            "processed_text": processed_text,
            "sentiment": sentiment_labels[predicted_class],
            "confidence": confidence,
            "probabilities": {
                "negative": float(probas[0][0]),
                "neutral": float(probas[0][1]),
                "positive": float(probas[0][2])
            },
            "prediction_time_ms": prediction_time * 1000
        }

    def save_unified(self, filepath='financial_sentiment_model_unified.pkl'):
        """Save the entire model, tokenizer, and vectorizer to a single file"""
        if not self.is_fitted:
            print("[ERROR] Cannot save an unfitted model")
            raise ValueError("Cannot save an unfitted model")

        print(f"[SAVE] Saving unified model to {filepath}")
        start_time = time.time()

        # First save the Keras model separately
        model_path = 'temp_model.h5'
        self.model.save(model_path)

        # Create a dictionary with all components
        model_package = {
            'preprocessor': self.preprocessor,
            'tokenizer': self.tokenizer,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'max_sequence_length': self.max_sequence_length,
            'max_vocab_size': self.max_vocab_size,
            'is_fitted': self.is_fitted
        }

        # Save using pickle
        with open(filepath, 'wb') as f:
            pickle.dump(model_package, f, protocol=4)

        # Copy the model file into the same directory for loading later
        # The .h5 file is still needed but will be handled automatically
        os.rename(model_path, filepath + '.h5')

        save_time = time.time() - start_time
        print(f"[SAVE] Unified model saved in {save_time:.2f} seconds")
        print(f"[SAVE] Unified model saved to {filepath} and {filepath}.h5")
        print("[SAVE] For API usage, you only need to load this single file")

    @classmethod
    def load_unified(cls, filepath='financial_sentiment_model_unified.pkl'):
        """Load the complete model from a single file"""
        print(f"[LOAD] Loading unified model from {filepath}")
        start_time = time.time()

        # Create a new instance
        model_instance = cls()

        # Load the package
        with open(filepath, 'rb') as f:
            model_package = pickle.load(f)

        # Restore components
        model_instance.preprocessor = model_package['preprocessor']
        model_instance.tokenizer = model_package['tokenizer']
        model_instance.tfidf_vectorizer = model_package['tfidf_vectorizer']
        model_instance.max_sequence_length = model_package['max_sequence_length']
        model_instance.max_vocab_size = model_package['max_vocab_size']
        model_instance.is_fitted = model_package['is_fitted']

        # Load the Keras model
        model_path = filepath + '.h5'
        model_instance.model = tf.keras.models.load_model(model_path)

        load_time = time.time() - start_time
        print(f"[LOAD] Unified model loaded in {load_time:.2f} seconds")
        print("[LOAD] Model loaded successfully!")

        return model_instance

"""# PROSES 6: TRAINING PIPELINE"""

# ===== SEL 6: TRAINING PIPELINE WITH CROSS-VALIDATION =====
def run_training_pipeline(data_path=Config.DATA_PATH):
    """Run the full training pipeline with stratified k-fold cross-validation"""
    print("="*80)
    print("[PIPELINE] STARTING FINANCIAL SENTIMENT ANALYSIS WITH STRATIFIED K-FOLD")
    print("="*80)

    # Set seeds for reproducibility
    np.random.seed(Config.SEED)
    tf.random.set_seed(Config.SEED)
    print("[PIPELINE] Random seeds set for reproducibility")

    # Load and prepare data
    df = pd.read_csv(data_path)
    print(f"\n[DATA LOADING] Loading data from {data_path}")
    print(f"[DATA LOADING] Dataset shape: {df.shape}")
    print(f"[DATA LOADING] Dataset columns: {', '.join(df.columns.tolist())}")
    print(f"[DATA LOADING] Total records: {len(df)}")
    start_time = time.time()
    load_time = time.time() - start_time
    print(f"[DATA LOADING] Dataset loaded in {load_time:.2f} seconds")

    # Display sample data
    print("\n[DATA PREVIEW] Sample data (first 5 rows):")
    print(df.head())

    df = df.dropna(subset=['Sentence', 'Sentiment'])  # Handle missing values
    X = df["Sentence"]
    y = df["Sentiment"].map({"negative":0, "neutral":1, "positive":2})

    print(f"[DATA LOADING] Dataset shape: {df.shape}")
    print(f"[DATA LOADING] Class distribution:\n{y.value_counts(normalize=True)}")

    # Check class distribution
    print("\n[DATA ANALYSIS] Class distribution:")
    class_dist = y.value_counts()
    for label, count in class_dist.items():
        print(f"  - {['Negative', 'Neutral', 'Positive'][label]}: {count} samples ({count/len(y)*100:.1f}%)")

    # Initialize Stratified K-Fold
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.SEED)
    fold_scores = []
    cm_accumulator = np.zeros((3,3))  # For confusion matrix
    best_model = None
    best_score = 0

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"\n{'='*40}")
        print(f"[FOLD {fold+1}/5]")
        print(f"{'='*40}")

        # Split data
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # Initialize and train model
        model = FinancialSentimentLSTM()
        start_train = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_train

        # Validate
        start_val = time.time()
        y_pred = model.predict(X_val)
        val_time = time.time() - start_val

        # Calculate metrics
        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='weighted')
        fold_scores.append((accuracy, f1))

        # Update best model
        if f1 > best_score:
            best_model = model
            best_score = f1

        # Accumulate confusion matrix
        cm_accumulator += confusion_matrix(y_val, y_pred)

        print(f"\n[FOLD {fold+1} RESULTS]")
        print(f"Training time: {train_time:.1f}s")
        print(f"Validation time: {val_time:.1f}s")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print(classification_report(y_val, y_pred, target_names=["negative", "neutral", "positive"]))

    # Cross-validation summary
    print("\n\n" + "="*80)
    print("[CROSS-VALIDATION SUMMARY]")
    avg_accuracy = np.mean([s[0] for s in fold_scores])
    avg_f1 = np.mean([s[1] for s in fold_scores])
    print(f"Average Accuracy: {avg_accuracy:.4f}")
    print(f"Average F1 Score: {avg_f1:.4f}")

    # Confusion matrix summary
    print("\n[SUMMARY CONFUSION MATRIX]")
    print(cm_accumulator.astype(int))
    print("\nRows: True labels, Columns: Predicted labels")
    print("Labels: [Negative, Neutral, Positive]")

    # Train final model on full data if needed
    # print("\n[TRAINING FINAL MODEL]")
    # final_model = FinancialSentimentLSTM()
    # final_model.fit(X, y)
    # best_model = final_model

    # Save best model
    print("\n[SAVING BEST MODEL]")
    best_model.save_unified()

    # Final evaluation on test set
    print("\n[FINAL EVALUATION]")
    test_idx = int(len(X) * 0.15)  # Hold-out 15% as test set
    X_test = X.iloc[:test_idx]
    y_test = y.iloc[:test_idx]

    y_pred = best_model.predict(X_test)
    print(classification_report(y_test, y_pred, target_names=["negative", "neutral", "positive"]))

    print("\n" + "="*80)
    print("[PIPELINE] TRAINING COMPLETED")
    print("="*80)

    return best_model

"""# PROSES 7: TESTING AND VISUALIZATION"""

# ===== SEL 7: TESTING AND VISUALIZATION =====
# Fungsi untuk menguji model dengan beberapa contoh kalimat

def test_prediction(model, texts):
    """Test the model with sample texts"""
    print("\n" + "="*80)
    print("[TEST RESULTS] SAMPLE PREDICTIONS WITH TRAINED MODEL")
    print("="*80)

    # Prepare a list to store all prediction results
    all_results = []

    for i, text in enumerate(texts):
        print(f"\n[TEST {i+1}/{len(texts)}] Text: \"{text}\"")

        # Timing the prediction
        start_time = time.time()
        proba = model.predict_proba([text])[0]
        pred_time = time.time() - start_time

        prediction = np.argmax(proba)
        sentiment = ["negative", "neutral", "positive"][prediction]
        confidence = proba[prediction]

        # Store result
        all_results.append({
            'text': text,
            'prediction': sentiment,
            'confidence': confidence,
            'probabilities': proba,
            'time': pred_time
        })

        # Pretty print results
        print(f"[RESULT] Predicted sentiment: {sentiment.upper()}")
        print(f"[RESULT] Confidence: {confidence:.2f} ({confidence*100:.1f}%)")
        print(f"[RESULT] Prediction time: {pred_time*1000:.2f} ms")
        print(f"[RESULT] Probability distribution:")
        print(f"         - Negative: {proba[0]:.4f} ({proba[0]*100:.1f}%)")
        print(f"         - Neutral:  {proba[1]:.4f} ({proba[1]*100:.1f}%)")
        print(f"         - Positive: {proba[2]:.4f} ({proba[2]*100:.1f}%)")

        # Visual representation of probabilities
        print("\n[VISUALIZATION] Sentiment probabilities:")
        bar_length = 40  # Adjust for longer or shorter bars
        for label, prob in zip(["Negative", "Neutral", "Positive"], proba):
            bar = "█" * int(prob * bar_length)
            empty = "░" * (bar_length - int(prob * bar_length))
            print(f"{label:8}: {bar}{empty} {prob:.4f} ({prob*100:.1f}%)")

    # Display aggregated stats
    print("\n" + "="*80)
    print("[SUMMARY] Prediction Statistics:")
    print(f"- Average prediction time: {np.mean([r['time']*1000 for r in all_results]):.2f} ms")
    print(f"- Average confidence: {np.mean([r['confidence'] for r in all_results]):.4f}")

    # Count predictions per class
    prediction_counts = Counter([r['prediction'] for r in all_results])
    print("\n[SUMMARY] Prediction distribution:")
    for sentiment, count in prediction_counts.items():
        print(f"- {sentiment.capitalize()}: {count} samples ({count/len(all_results)*100:.1f}%)")

    # Sample with highest confidence
    highest_conf = max(all_results, key=lambda x: x['confidence'])
    print(f"\n[SUMMARY] Sample with highest confidence ({highest_conf['confidence']:.4f}):")
    print(f"- Text: \"{highest_conf['text']}\"")
    print(f"- Prediction: {highest_conf['prediction'].upper()}")

    print("\n" + "="*80)

"""# PROSES 8: RUNNING THE MAIN PIPELINE"""

# ===== SEL 8: RUNNING THE MAIN PIPELINE =====
# Fungsi utama untuk menjalankan seluruh pipeline

def main():
    """Main function to run the improved financial sentiment analysis"""
    print("\n" + "="*80)
    print("[MAIN] STARTING IMPROVED FINANCIAL SENTIMENT ANALYSIS PIPELINE WITH TF-IDF")
    print("="*80)

    # Start timing
    total_start_time = time.time()

    # Train model
    train_start_time = time.time()
    model = run_training_pipeline(Config.DATA_PATH)
    train_time = time.time() - train_start_time
    print(f"\n[MAIN] Training pipeline completed in {train_time:.2f} seconds")

    # Test with sample financial texts
    print("\n[MAIN] Testing model with sample financial texts...")
    test_texts = [
        "The company reported strong earnings growth, exceeding analyst expectations for the third quarter in a row.",
        "The stock plunged 15% after the company announced disappointing quarterly results and lowered its guidance.",
        "The Federal Reserve kept interest rates unchanged, in line with market expectations.",
        "Investors are concerned about rising inflation and its potential impact on corporate profits.",
        "The merger is expected to create significant synergies and cost savings for both companies.",
        "Bank Indonesia mempertahankan suku bunga acuan pada level 5,75% pada rapat bulanan.",
        "Saham BBCA naik 2% setelah laporan keuangan kuartal 1 menunjukkan peningkatan laba."
    ]

    # Add time processing information
    print(f"[MAIN] Prepared {len(test_texts)} test samples")
    print("[MAIN] Sample text languages: English and Indonesian")

    # Run test prediction
    test_start_time = time.time()
    test_prediction(model, test_texts)
    test_time = time.time() - test_start_time
    print(f"[MAIN] Testing completed in {test_time:.2f} seconds")

    # Total pipeline time
    total_time = time.time() - total_start_time
    print(f"\n[MAIN] Complete pipeline execution time: {total_time:.2f} seconds")
    print("\n[MAIN] Improved financial sentiment analysis completed")
    print("="*80)

"""# PROSES 9: EXECUTE MAIN FUNCTION"""

# ===== SEL 9: EXECUTE MAIN FUNCTION =====
# Jalankan fungsi main ketika notebook dieksekusi

if __name__ == "__main__":
    main()

"""# TESTING"""

def extended_prediction_test(model, test_texts=None):
    """
    Fungsi pengujian model dengan banyak contoh teks (terutama Inggris) dan detail output yang diperkaya
    Params:
        model: Model FinancialSentimentLSTM yang sudah dilatih
        test_texts: Daftar teks yang akan diuji (opsional)
    """
    if test_texts is None:
        test_texts = [
            # Contoh Bahasa Inggris
            "The company announced record-breaking revenue growth of 23% in Q3.",
            "Analysts are concerned about the rising debt levels and potential bankruptcy risk.",
            "Stock prices fluctuated mildly amid mixed economic reports.",
            "The Fed's decision to raise interest rates could impact borrowing costs for businesses.",
            "Earnings reports showed a 15% increase in net profits year-over-year.",
            "Market sentiment turned bearish following disappointing GDP data.",
            "The merger is expected to generate significant synergies and cost reductions.",
            "Revenue missed expectations by 8% causing investor uncertainty.",
            "The company launched an innovative product line with promising market reception.",
            "Rising unemployment rates are causing consumer spending concerns.",
            "Apple Announces Major Share Buyback Amid Surging Revenue",
            "Federal Reserve Signals Confidence in Economic Recovery",
            "Retail Giant Faces Lawsuit Over Misleading Financial Statements",
            "Oil Prices Remain Stable Ahead of OPEC Meeting",
            "Crypto Market Wipes Out $300 Billion in Value Overnight",
            "Oil Prices Crash on Weak Global Demand Forecasts",
            "Mass Layoffs Announced by Tech Giant Following Earnings Miss",
        ]

    if not model.is_fitted:
        print("[ERROR] Model belum dilatih! Harap jalankan metode fit() terlebih dahulu.")
        print("[ERROR] Proses pengujian dibatalkan.")
        return

    print("\n" + "="*80)
    print("[EXTENDED TESTING] MULAI PENGETESAN MODEL SENTIMEN KEUANGAN")
    print(f"[EXTENDED TESTING] Jumlah sampel pengujian: {len(test_texts)}")
    print("="*80)

    try:
        # Inisialisasi statistik
        all_results = []
        start_total = time.time()

        # Uji setiap teks
        for idx, text in enumerate(test_texts):
            print(f"\n[TEST CASE {idx+1}/{len(test_texts)}] - \"{text}...\"")
            print("-"*60)
            print("[PROSES] Memulai analisis sentimen...")

            try:
                # Waktu eksekusi
                start = time.time()
                result = model.predict_single_text(text)
                elapsed = time.time() - start

                # Tampilkan detail hasil
                print(f"\n[HASIL] Sentimen: {result['sentiment'].upper()}")
                print(f"[HASIL] Kepercayaan: {result['confidence']:.2f} ({result['confidence']*100:.1f}%)")
                print(f"[HASIL] Waktu pemrosesan: {elapsed*1000:.2f} ms")
                print(f"\n[PROBABILITAS] Negative: {result['probabilities']['negative']:.4f}")
                print(f"[PROBABILITAS] Neutral:  {result['probabilities']['neutral']:.4f}")
                print(f"[PROBABILITAS] Positive: {result['probabilities']['positive']:.4f}")

                # Simpan hasil
                all_results.append(result)

                # Visualisasi dengan batang
                print("\n[VISUALISASI] Grafik probabilitas sentimen:")
                labels = ["Negative", "Neutral", "Positive"]
                values = [
                    result['probabilities']['negative'],
                    result['probabilities']['neutral'],
                    result['probabilities']['positive']
                ]
                for label, val in zip(labels, values):
                    bar = "█" * int(val * 40)  # Skala visual
                    print(f"{label}: {bar} {val:.2f}")

            except Exception as e:
                print(f"\n[ERROR] Gagal memproses teks: \"{text}\"")
                print(f"[ERROR] Pesan kesalahan: {str(e)}")
                # print(f"[TRACEBACK] {traceback.format_exc()}")

            print("-"*60)

        # Ringkasan statistik
        total_time = time.time() - start_total
        avg_time = total_time / len(test_texts) * 1000
        confidences = [r['confidence'] for r in all_results]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0

        print("\n" + "="*80)
        print("[RINGKASAN STATISTIK PENGUJIAN]")
        print(f"- Total waktu pemrosesan: {total_time:.2f} detik")
        print(f"- Waktu rata-rata per teks: {avg_time:.2f} ms")
        print(f"- Kepercayaan rata-rata: {avg_confidence:.4f}")
        print(f"- Jumlah prediksi: {len(all_results)}")

        # Distribusi prediksi
        sentiments = [r['sentiment'] for r in all_results]
        sentiment_counts = Counter(sentiments)
        print("\n[DISTRIBUSI SENTIMEN]:")
        for sentiment, count in sentiment_counts.items():
            print(f"- {sentiment.capitalize()}: {count} ({count/len(all_results)*100:.1f}%)")

        # Hasil dengan kepercayaan tertinggi
        if all_results:
            highest_conf = max(all_results, key=lambda x: x['confidence'])
            print("\n[HASIL TERBAIK]:")
            print(f"- Teks: \"{highest_conf['original_text'][:70]}...\"")
            print(f"- Sentimen: {highest_conf['sentiment'].upper()}")
            print(f"- Kepercayaan: {highest_conf['confidence']:.4f}")

        print("\n[STATUS] Pengujian model selesai!")
        print("="*80)

    except KeyboardInterrupt:
        print("\n[ABORT] Proses pengujian dibatalkan oleh pengguna.")
    except Exception as e:
        print(f"\n[CRITICAL ERROR] Kesalahan kritis saat pengujian: {str(e)}")
        # print(f"[TRACEBACK] {traceback.format_exc()}")

# Contoh penggunaan
if __name__ == "__main__":
    try:
        # Muat model terlatih
        print("[PROSES] Memuat model terlatih...")
        model = FinancialSentimentLSTM.load_unified("financial_sentiment_model_unified.pkl")
        print("[STATUS] Model dimuat dengan sukses!")

        # Jalankan pengujian
        extended_prediction_test(model)

    except FileNotFoundError:
        print("[ERROR] File model tidak ditemukan!")
        print("[SOLUSI] Pastikan file 'financial_sentiment_model_unified.pkl' ada di direktori ini.")
    except Exception as e:
        print(f"[ERROR] Kesalahan saat memuat model: {str(e)}")